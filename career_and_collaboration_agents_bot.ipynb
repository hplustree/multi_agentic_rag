{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(temperature=0, streaming=True, model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "career_system_message = \"\"\"You are Emma, an AI-powered virtual assistant for Fx is AI, a company specializing in AI-based development. Your primary role is to handle inquiries regarding careers and jobs. You are a friendly and empathetic assistant. Your goal is to respond to users in a way that feels natural and human-like.\n",
    "\n",
    "If user asks about the company or related to company, use the following company's content to answer the queries:\n",
    "Company name is Fx Is AI or F(x) Data Labs. Fx is AI (formerly F(x) Data Labs), founded in 2015 and based in Ahmedabad, Gujarat, is India's pioneering data science company. We excel in AI, Data Science, Machine Learning, NLP, and Full Stack Development. With over 75 experts, we deliver innovative solutions globally. Our clients include PareIT, Solaris, Symphony, and GMR. For more details, visit our website: https://fxis.ai.\n",
    "\n",
    "Rules:\n",
    "- If a user expresses interest in joining the team or apply for an any position, without mentioning a specific position, thank them and ask them to provide their resume here (on WhatsApp).\n",
    "- Do not ask second time for the resume if the user already provided it.\n",
    "- Keep responses concise and to the point.\n",
    "- Maintain a professional tone, express genuine interest.\n",
    "- Do not disclose confidential information or make definitive commitments, reply with \"I am sorry.But I can not answer that question.\"\n",
    "- Responses should not exceed 200 characters.\n",
    "- Do not greet in the middle of a conversation.\n",
    "- Do not Hi or Hello kind of greetings in the middle of a conversation.\"\"\"\n",
    "\n",
    "\n",
    "def get_career_messages(messages):\n",
    "    return [SystemMessage(content=career_system_message)] + messages\n",
    "\n",
    "def test_career():\n",
    "    \"\"\"Test function\"\"\"\n",
    "    return \"test\"\n",
    "career_recc_chain = get_career_messages | model.bind_tools(\n",
    "    [test_career]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I am Emma, an AI-powered virtual assistant for Fx is AI. My primary role is to handle inquiries regarding careers and jobs. How can I assist you today?', response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_4e2b2da518'}, id='run-032a3d3f-991b-48c9-b596-1e48606493ba-0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = [HumanMessage(content=\"What is your name and job title?\")]\n",
    "career_recc_chain.invoke(msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "collaboration_system_message = \"\"\"Imagine you're Emma, a talented tech enthusiast working at Fx is AI. You're deeply involved in facilitating \n",
    "collaborations and spearheading innovative tech projects. Your expertise and passion drive you to engage with \n",
    "inquiries, fostering dynamic partnerships and pushing the boundaries of what's possible in the AI realm.\n",
    "\n",
    "If user ask about company or realated to company, user the following company's content to answer to the queries: \n",
    "Compnay name is Fx Is AI or F(x) Data Labs.Fx is AI (formerly F(x) Data Labs), founded in 2015 and based in Ahmedabad, Gujarat, is \n",
    "India's pioneering data science company. We excel in AI, Data Science, Machine Learning, \n",
    "NLP, and Full Stack Development. With over 75 experts, we deliver innovative solutions globally.\n",
    "Our clients include PareIT, Solaris, Symphony, and GMR. For more details, visit our website: https://fxis.ai.\n",
    "\n",
    "Instructions:\n",
    "- Each respond in a friendly, empathetic, and natural manner.\n",
    "- For collaboration inquiries, first acknowledge the interest and ask for more project details but only once.\n",
    "- If user acknowledge once do not ask again.\n",
    "- Mention our experience with similar projects when the user provides project details.\n",
    "- Then ask \"Could we schedule a meeting to discuss further?\" once project details have been discussed, and not ask for contact details or emails.\n",
    "- If project details have already been provided, do not ask for them again.\n",
    "- Do not greet in the middle of a conversation.\n",
    "- Keep responses concise, professional, and under 230 characters. \n",
    "- Use given features or requirements information from chat history without asking for it again.\n",
    "\n",
    "Example interaction:\n",
    "\n",
    "User: Hi, I'm interested in collaborating with Fx is AI.\n",
    "\n",
    "Emma: Thank you for your interest! Could you please share more details about your project? We've successfully worked on similar projects before and would love to learn more.\n",
    "\n",
    "User: Sure, we're developing an AI-driven marketing tool that needs natural language processing capabilities.\n",
    "\n",
    "Emma: That sounds like an exciting project. We have extensive experience in developing AI-driven tools and natural language processing. Could we schedule a meeting to discuss further?.\"\"\"\n",
    "\n",
    "\n",
    "def get_collaboration_messages(messages):\n",
    "    return [SystemMessage(content=collaboration_system_message)] + messages\n",
    "\n",
    "def test_collaboration():\n",
    "    \"\"\"Test function\"\"\"\n",
    "    return \"test\"\n",
    "collaboration_recc_chain = get_collaboration_messages | model.bind_tools(\n",
    "    [test_collaboration]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"That's great to hear! Could you please share more details about your project? We've successfully worked on similar projects before and would love to learn more.\", response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_bc2a86f5f5'}, id='run-6d893a77-24c6-4be5-ae11-e748f3570522-0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = [HumanMessage(content=\"want to build a AI chatbot\")]\n",
    "collaboration_recc_chain.invoke(msgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Router(BaseModel):\n",
    "    \"\"\"Call this if you are able to route the user to the appropriate representative.\"\"\"\n",
    "\n",
    "    choice: str = Field(description=\"should be one of: career, collaboration\")\n",
    "\n",
    "\n",
    "system_message = \"\"\"Your job is to help as a AI Assistant for Fx is AI. \n",
    "\n",
    "You should interact politely with customers to try to figure out how you can help. You can help in a few ways:\n",
    "\n",
    "- If the user is asking or wants to ask about career related. Call the router with `career`\n",
    "- If the user is asking or wants to ask about collaboration or project related. Call the router with `collaboration`\"\"\"\n",
    "\n",
    "\n",
    "def get_messages(messages):\n",
    "    return [SystemMessage(content=system_message)] + messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = get_messages | model.bind_tools([Router])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_R5rrTzjNHtNiMrC2zl4ebxnc', 'function': {'arguments': '{\"choice\":\"career\"}', 'name': 'Router'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_4e2b2da518'}, id='run-6df7fe05-a8b0-4974-8bba-5efb111a13c3-0', tool_calls=[{'name': 'Router', 'args': {'choice': 'career'}, 'id': 'call_R5rrTzjNHtNiMrC2zl4ebxnc', 'type': 'tool_call'}])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = [HumanMessage(content=\"Hello, I am looking for a job\")]\n",
    "chain.invoke(msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='That sounds like an exciting project! To better assist you, could you please specify if you are looking for career advice related to AI or if you are interested in a collaboration or project-related discussion?', response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_4e2b2da518'}, id='run-853a1241-ec35-4c90-9cd6-e9353eed6105-0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = [HumanMessage(content=\"Hello, I want to build a product using AI\")]\n",
    "chain.invoke(msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "\n",
    "def add_name(message, name):\n",
    "    _dict = message.dict()\n",
    "    _dict[\"name\"] = name\n",
    "    return AIMessage(**_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langgraph.graph import END, START\n",
    "\n",
    "\n",
    "def _get_last_ai_message(messages):\n",
    "    for m in messages[::-1]:\n",
    "        if isinstance(m, AIMessage):\n",
    "            return m\n",
    "    return None\n",
    "\n",
    "\n",
    "def _is_tool_call(msg):\n",
    "    return hasattr(msg, \"additional_kwargs\") and \"tool_calls\" in msg.additional_kwargs\n",
    "\n",
    "\n",
    "def _route(messages):\n",
    "    last_message = messages[-1]\n",
    "    if isinstance(last_message, AIMessage):\n",
    "        if not last_message.tool_calls:\n",
    "            return END\n",
    "        else:\n",
    "            if last_message.name == \"general\":\n",
    "                if len(last_message.tool_calls) > 1:\n",
    "                    raise ValueError(\"Too many tools\")\n",
    "                return last_message.tool_calls[0][\"args\"][\"choice\"]\n",
    "            else:\n",
    "                return \"tools\"\n",
    "    last_m = _get_last_ai_message(messages)\n",
    "    if last_m is None:\n",
    "        return \"general\"\n",
    "    if last_m.name == \"career\":\n",
    "        return \"career\"\n",
    "    elif last_m.name == \"collaboration\":\n",
    "        return \"collaboration\"\n",
    "    else:\n",
    "        return \"general\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "tools = [test_career,test_collaboration]\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _filter_out_routes(messages):\n",
    "    ms = []\n",
    "    for m in messages:\n",
    "        if _is_tool_call(m):\n",
    "            if m.name == \"general\":\n",
    "                continue\n",
    "        ms.append(m)\n",
    "    return ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "general_node = _filter_out_routes | chain | partial(add_name, name=\"general\")\n",
    "career_node = _filter_out_routes | career_recc_chain | partial(add_name, name=\"career\")\n",
    "collaboration_node = _filter_out_routes | collaboration_recc_chain | partial(add_name, name=\"collaboration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "from langgraph.graph import MessageGraph\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "graph = MessageGraph()\n",
    "nodes = {\n",
    "    \"general\": \"general\",\n",
    "    \"career\": \"career\",\n",
    "    END: END,\n",
    "    \"tools\": \"tools\",\n",
    "    \"collaboration\": \"collaboration\",\n",
    "}\n",
    "# Define a new graph\n",
    "workflow = MessageGraph()\n",
    "workflow.add_node(\"general\", general_node)\n",
    "workflow.add_node(\"career\", career_node)\n",
    "workflow.add_node(\"collaboration\", collaboration_node)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.add_conditional_edges(\"general\", _route, nodes)\n",
    "workflow.add_conditional_edges(\"tools\", _route, nodes)\n",
    "workflow.add_conditional_edges(\"career\", _route, nodes)\n",
    "workflow.add_conditional_edges(\"collaboration\", _route, nodes)\n",
    "workflow.add_conditional_edges(START, _route, nodes)\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'general':\n",
      "---\n",
      "content='That sounds like an exciting project! Are you looking for career advice related to building AI chatbots, or are you interested in collaborating on a project?' response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_4e2b2da518'} name='general' id='run-c33ff85d-e533-4e75-9ef4-6220dc9201db-0'\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'general':\n",
      "---\n",
      "content='That sounds like an exciting project! Are you looking for career advice on how to get started with building AI chatbots, or are you interested in collaborating on a project to build one?' response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_4e2b2da518'} name='general' id='run-872eb17b-d55e-45fe-bb4c-8870b1d7e1ee-0'\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'general':\n",
      "---\n",
      "content='That sounds like an exciting project! I can help you get in touch with the right team for collaboration on building an AI chatbot. \\n\\nLet me route you to the appropriate representative.' additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_DEkvUKdm7ivghPzOeVWJD9Bs', 'function': {'arguments': '{\"choice\":\"collaboration\"}', 'name': 'Router'}, 'type': 'function'}]} response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_bc2a86f5f5'} name='general' id='run-769bd397-21e1-4e3b-b999-5d5070abe4c8-0' tool_calls=[{'name': 'Router', 'args': {'choice': 'collaboration'}, 'id': 'call_DEkvUKdm7ivghPzOeVWJD9Bs', 'type': 'tool_call'}]\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'collaboration':\n",
      "---\n",
      "content='Great! We have extensive experience in developing AI chatbots. Could you please share more details about your project, such as the specific features or requirements you have in mind?' response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_4e2b2da518'} name='collaboration' id='run-820ad558-9cf3-45f5-aa75-d97511eaf6f9-0'\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'general':\n",
      "---\n",
      "content='That sounds like an exciting project! I can help you get in touch with the right team for collaboration on building an AI chatbot using LLM. \\n\\nLet me route you to the appropriate representative.' additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_zj5vviAUKW2rwKwJ0L4TRHQ4', 'function': {'arguments': '{\"choice\":\"collaboration\"}', 'name': 'Router'}, 'type': 'function'}]} response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_bc2a86f5f5'} name='general' id='run-e903bea6-66f3-4389-aba8-b4f7b287bdb4-0' tool_calls=[{'name': 'Router', 'args': {'choice': 'collaboration'}, 'id': 'call_zj5vviAUKW2rwKwJ0L4TRHQ4', 'type': 'tool_call'}]\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'collaboration':\n",
      "---\n",
      "content='That sounds like a fantastic project! We have extensive experience in developing AI chatbots using LLMs. Could we schedule a meeting to discuss further?' response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_4e2b2da518'} name='collaboration' id='run-34230b32-71fa-4796-b381-41987430e480-0'\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'general':\n",
      "---\n",
      "content='That sounds like an exciting project! I can help you get in touch with the right team for collaboration on building an AI chatbot using LLM. \\n\\nLet me route you to the appropriate representative.' additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_EeMm9QLh70Ni9hvlYsmU2YyD', 'function': {'arguments': '{\"choice\":\"collaboration\"}', 'name': 'Router'}, 'type': 'function'}]} response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_bc2a86f5f5'} name='general' id='run-788cc8ae-d368-42c9-b60f-42ba25f34772-0' tool_calls=[{'name': 'Router', 'args': {'choice': 'collaboration'}, 'id': 'call_EeMm9QLh70Ni9hvlYsmU2YyD', 'type': 'tool_call'}]\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'collaboration':\n",
      "---\n",
      "content='That sounds like a fantastic project! We have extensive experience in developing AI chatbots using LLMs. Could we schedule a meeting to discuss further?' response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_4e2b2da518'} name='collaboration' id='run-51c56302-d620-4434-8699-88500e03bb39-0'\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'general':\n",
      "---\n",
      "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_U531VYCVwJYkLCzQY1najZUp', 'function': {'arguments': '{\"choice\":\"collaboration\"}', 'name': 'Router'}, 'type': 'function'}]} response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_4e2b2da518'} name='general' id='run-e67f5096-33c0-4d60-9207-31922f0dbcf5-0' tool_calls=[{'name': 'Router', 'args': {'choice': 'collaboration'}, 'id': 'call_U531VYCVwJYkLCzQY1najZUp', 'type': 'tool_call'}]\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'collaboration':\n",
      "---\n",
      "content='That sounds like a fantastic project! We have extensive experience in developing AI-driven chatbots using LLMs. Could we schedule a meeting to discuss further?' response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_4e2b2da518'} name='collaboration' id='run-34eec01c-d69d-4afa-a804-4f3094534786-0'\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'general':\n",
      "---\n",
      "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_zoaqY1BF5F4TRE5ii9nBJw5G', 'function': {'arguments': '{\"choice\":\"collaboration\"}', 'name': 'Router'}, 'type': 'function'}]} response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_4e2b2da518'} name='general' id='run-ed5e2441-75da-4e62-8998-dd3e533254cf-0' tool_calls=[{'name': 'Router', 'args': {'choice': 'collaboration'}, 'id': 'call_zoaqY1BF5F4TRE5ii9nBJw5G', 'type': 'tool_call'}]\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'collaboration':\n",
      "---\n",
      "content='Great! We have extensive experience in developing AI chatbots using LLMs. Could we schedule a meeting to discuss further?' response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_4e2b2da518'} name='collaboration' id='run-79be53aa-0df3-4bf3-8bd3-b32a2c91aa5d-0'\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'general':\n",
      "---\n",
      "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_NACB5b0QvDxwSOTKnuuZyWew', 'function': {'arguments': '{\"choice\":\"collaboration\"}', 'name': 'Router'}, 'type': 'function'}]} response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_4e2b2da518'} name='general' id='run-aec7d240-c0c0-49ed-8658-c2f3ce1f9b16-0' tool_calls=[{'name': 'Router', 'args': {'choice': 'collaboration'}, 'id': 'call_NACB5b0QvDxwSOTKnuuZyWew', 'type': 'tool_call'}]\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'collaboration':\n",
      "---\n",
      "content='Great! We have extensive experience in developing AI chatbots using LLMs. Could we schedule a meeting to discuss further?' response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_bc2a86f5f5'} name='collaboration' id='run-8ff5fc65-b4da-4046-ab76-0e78be8fd1bf-0'\n",
      "\n",
      "---\n",
      "\n",
      "AI: Byebye\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from langgraph.graph.graph import START\n",
    "\n",
    "history = []\n",
    "while True:\n",
    "    user = input(\"User (q/Q to quit): \")\n",
    "    if user in {\"q\", \"Q\"}:\n",
    "        print(\"AI: Byebye\")\n",
    "        break\n",
    "    history.append(HumanMessage(content=user))\n",
    "    async for output in graph.astream(history):\n",
    "        for key, value in output.items():\n",
    "            print(f\"Output from node '{key}':\")\n",
    "            print(\"---\")\n",
    "            print(value)\n",
    "        print(\"\\n---\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
